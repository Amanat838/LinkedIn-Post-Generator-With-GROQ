[
	{
		"text": "Just saw a LinkedIn Influencer with 'Organic Growth' written in the profile with 65K+ followers claiming that he can help you in growing your platform, copying the posts from other influencers.",
		"engagement": 90,
		"Influencer": "Muskan Handa"
	},
	{	
		"text": "Jobseekers, this one‚Äôs for you.\n Every application, every interview, every follow-up‚Ä¶ the pressure is immense.\n And I know what you're thinking: Am I not good enough? \n But let me tell you, this isn‚Äôt about you or your skills. It‚Äôs about a broken system where 60% of applicants never hear back. \n Your mental health is not worth sacrificing for a system that doesn‚Äôt acknowledge your worth. \n Please remember, taking care of yourself is the real priority. \n Your dream job will come, but for now, breathe. üåª",
		"engagement": 347,
		"Influencer": "Muskan Handa"
	},
	{
		"text": "Looking for jobs on LinkedIn is like online dating: Full of promises, but in the end, you‚Äôre just left ghosted.",
		"engagement": 109,
		"Influencer": "Muskan Handa"
	},
	{
		"text": "LinkedIn scams be like: 'Congratulations, you've been selected for a role you didn‚Äôt even apply for!' \n The catch? Pay Rs. 50,000 for the honor.",
		"engagement": 115,
		"Influencer": "Muskan Handa"
	},
	{
		"text": "sapne dekhna achi baat hai,\nlekin job ka sapna dekh ke 'interested' likhna,\nyeh toh achi baat nahi hai na?",
		"engagement": 545,
		"Influencer": "Muskan Handa"
	},
	{
		"text": "Next time when I'll be reading some LinkedIn Influencer's story, I am starting from the last line.\nIf there's a link attached to it, it's most probably a fake one.\nSaves me time!",
		"engagement": 188,
		"Influencer": "Muskan Handa"
	},
	{
		"text": "Every time I poured my heart into 5-6 rounds of interviews and faced rejection, it felt like a punch in the gut. The sleepless nights, the endless preparation, all for nothing.\n\nBut looking back, I realize it wasn‚Äôt nothing. It was the Universe‚Äôs way of saying, ‚ÄúNot this one, something better is on the way.‚Äù\n\nEvery single time, I‚Äôve been shown why that rejection happened.\n\nDoors I thought I wanted to walk through were shut, only to have the right ones swing open.\n\nThe kind that aligned with my growth, my values, and my happiness.\n\nAt first, it stung. It hurt deeply. But now, when things don‚Äôt go as planned, I don‚Äôt panic.\n\nI don‚Äôt question my worth. I sit back, breathe, and trust. The Universe knows.\n\nI know there's another plan waiting. Something bigger, better, and just for me.\n\nTo anyone feeling the weight of rejection: trust that the closed doors are protecting you from something you can‚Äôt see right now.\n\nYour path is being cleared for something even more beautiful.",
		"engagement": 206,
		"Influencer": "Muskan Handa"
	},
	{
		"text": "To everyone who's still looking for a job...\n\nI see you. I feel you. \ud83d\udc94\n\nEvery rejection email feels like a punch in the gut, and every 'We'll get back to you' sounds more like 'You'll never hear from us.'\n\nBut I want you to know, you're not alone in this. \ud83c\udf38\n\nAccording to a study, 80% of jobseekers struggle with anxiety and self-doubt during their search. It's normal to feel lost, but it's not the end.\n\nTake breaks, breathe, and remember, this doesn't define you. Your worth is not tied to an offer letter. \ud83d\udca5\n\nYour mental health matters more than any job.",
		"engagement": 899,
		"Influencer": "Muskan Handa"
	},
	{
		"text": "Sometimes, we forget that a company‚Äôs brand name doesn‚Äôt define someone‚Äôs talent. It‚Äôs easy to get caught up in the 'big company = big talent' mindset, but that's not always the case.\n\nI‚Äôve had the privilege of working with people from smaller companies (lesser known) who blow me away with their skills and dedication. They don‚Äôt need a fancy title or a famous brand behind them to prove their worth.\n\nI've seen the other side too‚Äîpeople in top-tier companies feeling lost, overwhelmed, or stuck, even though the world sees them as 'successful'.\n\nLet‚Äôs stop attaching someone‚Äôs value to the company they work for. Freshers especially need to hear this‚Äîskills are what matter, not the size of the company behind them.\n\nAt the end of the day, happiness and growth don‚Äôt come from a brand name, they come from doing what you love and constantly improving your craft.",
		"engagement": 166,
		"Influencer": "Muskan Handa"
	},
	{
		"text": "So when I left a toxic work environment, I told my manager a simple thing and felt so good \ud83d\ude2f\n\nI just said-\n\n'Hope your son gets a manager like you.\nI hope that the manager behaves the same way as you did with me.\nThank you.'\n\nNow tell me 1 thing-\n\nShe always said that she was a great manager.\nWhy will she get offended?\n\nI just told her that I wish her son would get a manager like she was.\n\nIf you felt bad, then that means you were a bad manager and now you know it. \ud83e\b80\n\nIf you feel good, then take it as a blessing for your son and you'll really want someone to treat your son/daughter in the same way.\n\nShe cannot be even angry with me else it'll prove that she was not a 'great' manager.\n\nMuskan - 1\nManager - 0\n\nMuskan -> Aura +100000000\n\n(Fictional message unfortunately :(\n)\n\nHope you all become the people that your sons/daughters will like to work under \ud83d\ude4f\n\nThere are a lot of bad people/things, bring a small change and break the chain :)",
		"engagement": 1111,
		"Influencer": "Muskan Handa"
	},
	{
		"text": "There's a new kind of coding I call 'vibe coding', where you fully give in to the vibes, embrace exponentials, and forget that the code even exists. It's possible because the LLMs (e.g. Cursor Composer w Sonnet) are getting too good. Also I just talk to Composer with SuperWhisper so I barely even touch the keyboard. I ask for the dumbest things like 'decrease the padding on the sidebar by half' because I'm too lazy to find it. I 'Accept All' always, I don't read the diffs anymore. When I get error messages I just copy paste them in with no comment, usually that fixes it. The code grows beyond my usual comprehension, I'd have to really read through it for a while. Sometimes the LLMs can't fix a bug so I just work around it or ask for random changes until it goes away. It's not too bad for throwaway weekend projects, but still quite amusing. I'm building a project or webapp, but it's not really coding - I just see stuff, say stuff, run stuff, and copy paste stuff, and it mostly works.",
		"engagement": 1000,
		"Influencer": "Andrej Karpathy"
	},
	{
		"text": "Some personal news: I am joining OpenAI (again :)). Like many others both in/out of AI, I am very inspired by the impact of their work and I have personally benefited greatly from it. The future potential is especially exciting; it is a great pleasure to jump back in and build!",
		"engagement": 1007,
		"Influencer": "Andrej Karpathy"
	},
	{
		"text": "Agency > Intelligence I had this intuitively wrong for decades, I think due to a pervasive cultural veneration of intelligence, various entertainment/media, obsession with IQ etc. Agency is significantly more powerful and significantly more scarce. Are you hiring for agency? Are we educating for agency? Are you acting as if you had 10X agency? Grok explanation is ~close: ‚ÄúAgency, as a personality trait, refers to an individual's capacity to take initiative, make decisions, and exert control over their actions and environment. It‚Äôs about being proactive rather than reactive‚Äîsomeone with high agency doesn‚Äôt just let life happen to them; they shape it. Think of it as a blend of self-efficacy, determination, and a sense of ownership over one‚Äôs path. People with strong agency tend to set goals and pursue them with confidence, even in the face of obstacles. They‚Äôre the type to say, ‚ÄúI‚Äôll figure it out,‚Äù and then actually do it. On the flip side, someone low in agency might feel more like a passenger in their own life, waiting for external forces‚Äîlike luck, other people, or circumstances‚Äîto dictate what happens next. It‚Äôs not quite the same as assertiveness or ambition, though it can overlap. Agency is quieter, more internal‚Äîit‚Äôs the belief that you *can* act, paired with the will to follow through. Psychologists often tie it to concepts like locus of control: high-agency folks lean toward an internal locus, feeling they steer their fate, while low-agency folks might lean external, seeing life as something that happens to them.‚Äù",
		"engagement": 7856,
		"Influencer": "Andrej Karpathy"
	},
	{
		"text": "New 3h31m video on YouTube: 'Deep Dive into LLMs like ChatGPT' This is a general audience deep dive into the Large Language Model (LLM) AI technology that powers ChatGPT and related products. It is covers the full training stack of how the models are developed, along with mental models of how to think about their 'psychology', and how to get the best use them in practical applications. We cover all the major stages: 1. pretraining: data, tokenization, Transformer neural network I/O and internals, inference, GPT-2 training example, Llama 3.1 base inference examples 2. supervised finetuning: conversations data, 'LLM Psychology': hallucinations, tool use, knowledge/working memory, knowledge of self, models need tokens to think, spelling, jagged intelligence 3. reinforcement learning: practice makes perfect, DeepSeek-R1, AlphaGo, RLHF. I designed this video for the 'general audience' track of my videos, which I believe are accessible to most people, even without technical background. It should give you an intuitive understanding of the full training pipeline of LLMs like ChatGPT, with many examples along the way, and maybe some ways of thinking around current capabilities, where we are, and what's coming. (Also, I have one 'Intro to LLMs' video already from ~year ago, but that is just a re-recording of a random talk, so I wanted to loop around and do a lot more comprehensive version of this topic. They can still be combined, as the talk goes a lot deeper into other topics, e.g. LLM OS and LLM Security)",
		"engagement": 107,
		"Influencer": "Andrej Karpathy"
	},
	{
		"text": "DeepSeek (Chinese AI co) making it look easy today with an open weights release of a frontier-grade LLM trained on a joke of a budget (2048 GPUs for 2 months, $6M). For reference, this level of capability is supposed to require clusters of closer to 16K GPUs, the ones being brought up today are more around 100K GPUs. E.g. Llama 3 405B used 30.8M GPU-hours, while DeepSeek-V3 looks to be a stronger model at only 2.8M GPU-hours (~11X less compute). If the model also passes vibe checks (e.g. LLM arena rankings are ongoing, my few quick tests went well so far) it will be a highly impressive display of research and engineering under resource constraints. Does this mean you don't need large GPU clusters for frontier LLMs? No but you have to ensure that you're not wasteful with what you have, and this looks like a nice demonstration that there's still a lot to get through with both data and algorithms. Very nice & detailed tech report too, reading through.",
		"engagement": 7854,
		"Influencer": "Andrej Karpathy"
	},
	{
		"text": "Once again, an AI system is not 'thinking', it's 'processing', 'running predictions',... just like Google or computers do. Giving the false impression that technology systems are human is just cheap snake oil and marketing to fool you into thinking it's more clever than it is.",
		"engagement": 107,
		"Influencer": "ClementDelangue"
	},
	{
		"text": "Great research on open-source by @Harvard - $4.15B invested in open-source generates $8.8T of value for companies (aka $1 invested in open-source = $2,000 of value created) - Companies would need to spend 3.5 times more on software than they currently do if OSS did not exist I suspect that these numbers and impact are even greater for AI than for software (would be great to study!)",
		"engagement": 145,
		"Influencer": "ClementDelangue"
	},
	{
		"text": "Just 10 days after o1's public debut, we‚Äôre thrilled to unveil the open-source version of the groundbreaking technique behind its success: scaling test-time compute üß†üí° By giving models more 'time to think' LLaMA 1B outperforms LLaMA 8B in math‚Äîbeating a model 8x its size. The full recipe is open-sourceü§Ø This is the power of open science and open-source AI! üåç‚ú®",
		"engagement": 1032,
		"Influencer": "ClementDelangue"
	},
	{
		"text": "Our science team has started working on fully reproducing and open-sourcing R1 including training data, training scripts,... Full power of open source AI so that everyone all over the world can take advantage of AI progress! Will help debunk some myths I‚Äôm sure too. Thanks @deepseek_ai!",
		"engagement": 6332,
		"Influencer": "ClementDelangue"
	},
	{
		"text": "Six predictions for AI in 2024: - A hyped AI company will go bankrupt or get acquired for a ridiculously low price - Open-source LLMs will reach the level of the best closed-source LLMs - Big breakthroughs in AI for video, time-series, biology and chemistry - We will talk much more about the cost (monetary and environmental) of AI - A popular media will be mostly AI-generated - 10 millions AI builders on Hugging Face leading to no increase of unemployment",
		"engagement": 7888,
		"Influencer": "ClementDelangue"
	},
	{
		"text": "Had a look through @Grok's code: 1. Attention is scaled by 30/tanh(x/30) ?! 2. Approx GELU is used like Gemma 3. 4x Layernoms unlike 2x for Llama 4. RMS Layernorm downcasts at the end unlike Llama - same as Gemma 5. RoPE is fully in float32 I think like Gemma 6. Multipliers are 1 7. QKV has bias, O no bias MLP no bias 8. Vocab size is 131072. Gemma 256000. Sadly, it's way too large for me to accelerate finetuning or running with @UnslothAI. 70b Llama fits in 48GB with Unsloth, but Grok will need way too many GPUs :(",
		"engagement": 3000,
		"Influencer": "Daniel Han"
	},
	{
		"text": "Found more bugs for #Gemma: 1. Must add <bos> 2. There‚Äôs a typo for <end_of_turn>model 3. sqrt(3072)=55.4256 but bfloat16 is 55.5 4. Layernorm (w+1) must be in float32 5. Keras mixed_bfloat16 RoPE is wrong 6. RoPE is sensitive to y*(1/x) vs y/x 7. (Fixed) RoPE should be float32 8. (PR) GELU should be approx tanh not exact We already pushed all fixes to @UnslothAI. Using the @GoogleDeepMind impl, I compared the Log L2 norms across layers. Each line is a fix we did.",
		"engagement": 179,
		"Influencer": "Daniel Han"
	},
	{
		"text": "Fixed a bug which caused all training losses to diverge for large gradient accumulation sizes. 1. First reported by @bnjmn_marie, GA is supposed to be mathematically equivalent to full batch training, but losses did not match. 2. We reproed the issue, and further investigation showed the L2 Norm betw bsz=16 and ga=16 was 10x larger. 3. The culprit was the cross entropy loss normalizer. 4. We ran training runs with denormalized CE Loss, and all training losses match. 5. We then re-normalized CE Loss with the correct denominator across all gradient accumulation steps, and verified all training loss curves match now. 6. We've already updated @UnslothAI with the fix, and wrote up more details in our blog post here: http://unsloth.ai/blog/gradient This issue impacts all libraries which use GA, and simple averaging of GA does not work for varying sequence lengths. This also impacts DDP and multi GPU training which accumulates gradients. Please update Unsloth via pip install --upgrade --no-cache-dir unsloth and use from unsloth import unsloth_train",
		"engagement": 896,
		"Influencer": "Daniel Han"
	},
	{
		"text": "Was fixing LLM fine-tuning bugs and found 4 issues: 1. Mistral: HF's batch_decode output is wrong 2. Llama-3: Be careful of double BOS 3. Gemma: 2nd token has an extra space - GGUF(_Below) = 30641 vs HF(Below) = 33501 4. Gemma-it: Also be careful of double BOS Solutions: 1. Mistral - should be fine, but individual tokens via batch_decode is wrong - when using batch_decode, check the spaces in front of each token. 2. Llama-3: check all chat templates and use --verbose-prompt in llama.cpp to check for double BOS tokens. @UnslothAI community members first noticed about this issue. Also be very careful during fine-tuning - we're adding an auto check in Unsloth in the next release. 3. Gemma: 1 token id difference after <bos>. Unsure whether (1) extra space or (2) no extra space is correct - luckily not major & working with the Gemma team to resolve this! 4. Gemma-it: Same issue as Llama-3 - check chat templates for double BOS tokens.",
		"engagement": 2141,
		"Influencer": "Daniel Han"
	},
	{
		"text": "Just analyzed Google's new Gemma 2 release! The base and instruct for 9B & 27B is here! 1. Pre & Post Layernorms = x2 more LNs like Grok 2. Uses Grok softcapping! Attn logits truncated to (-30, 30) & logits (-50, 50) 3. Alternating 4096 sliding window like Mistral & 8192 global attn! SWA->Global->SWA->Global etc 4. Attention is scaled like Grok - instead of 1/sqrt(d), does 1/sqrt(s). 9B uses 1/sqrt(224) not 1/sqrt(256). 27B uses 1/sqrt(144) not 1/sqrt(128) 5. Approx GeGLU - great it's clear re our past bug fixes! 6. Be careful of float casting like in our prev bug fixes! 7. 3 sizes: 2.6B, 9B, 27B. 2.6B released later. MMLU for Base models very strong, and Instruct also great 8. Vocab size of 256,128 for all variants",
		"engagement": 200,
		"Influencer": "Daniel Han"
	},
	{
		"text": "The tech job market is changing, and it‚Äôs not just a short term effect. These shifts have long-term implications. A recent ‚ÄúState of Talent Report - 2025‚Äù report from SignalFire shared some interesting insights. I‚Äôm often asked, ‚ÄúWhich certification is good for AI?‚Äù My counter-question is: 'Does it even matter anymore?'. Your public presence on GitHub, LinkedIn, or Medium is becoming increasingly important to showcase your portfolio and signal your capabilities. A college degree no longer provides the leverage it once did. That doesn‚Äôt mean it‚Äôs obsolete, but it‚Äôs becoming more of a checkbox than a differentiator. As the pace of execution accelerates, the time available for formal training is shrinking. Don‚Äôt confuse this with upskilling or retraining; those still matter. What‚Äôs different is that many junior roles are now being filled by experienced individual contributors (ICs). It‚Äôs not uncommon to see senior ICs take a level cut to pivot into AI roles. This shift is creating a new ‚Äúlevel conundrum‚Äù in organizations. We‚Äôre likely to see even more pressure in the near to medium term. I usually see generic advice like ‚Äúbuild AI skills‚Äù. If you‚Äôre a recent or soon-to-be graduate, my suggestion is this: Build a portfolio in AI and share your learning journey publicly. Write blogs or articles explaining the solutions you've built. Pitch those solutions and ask for collaborations from industry experts to build visibility. Focus on building a reciprocal, value-adding network. Your competition is not just your peers but also your seniors. Don‚Äôt just collect skills or connections; build leverage.",
		"engagement": 5000,
		"Influencer": "Nitin Aggarwal"
	},
	{
		"text": "I‚Äôm bullish on two key aspects of AI evolution: AgenticWeb and AgenticOS. The web and OS were originally developed with humans (or human experience) in mind, but the way AI agents are evolving, these systems can no longer be consumed as-is. Microsoft‚Äôs initiative around NLWeb is a good read. It‚Äôs fun to see turning any website into an AI-powered app and eventually integrating it into an agentic system. It reminds me of the early days of the internet, when we saw the rise of protocols like TCP/IP and HTTP. Today, we're witnessing something similar with MCP, A2A, NLWeb (not as such a protocol), etc. The way humans interact with websites is about to change drastically. We‚Äôre moving toward a world where websites become more like static catalogs similar to old telephone directories. They‚Äôll serve primarily as structured sources of information that humans consume via agents to make decisions, rather than destinations for human visits. Excited to see a ‚Äúhuman-centric‚Äù design evolving to ‚Äúagent-centric‚Äù design in the near future. It feels like we have learned a lot about how humans think; it‚Äôs time to learn how machines (‚Äúagents‚Äù) feel. hashtag#ExperienceFromTheField hashtag#WrittenByHuman",
		"engagement": 500,
		"Influencer": "Nitin Aggarwal"
	},
	{
		"text": "AI took a significant leap to evolve from machine learning (ML) to deep learning (DL). I still remember the famous circular diagram that explained AI as a field, ML as a domain within it, and DL as a sub-domain of ML. But lately, the taxonomy feels increasingly confusing and subjective. I recently read a paper discussing AI agents versus Agentic AI, and it also tried to differentiate Generative AI from both. The paper came from a reputable source, and I read it closely. However, the definitions felt force-fitted to the use cases just to highlight differences. Based on my understanding of the paper, generating content with a simple prompt is labeled Generative AI, generating content to post on LinkedIn is called an AI agent; automatically posting the content to LinkedIn becomes Agentic AI. Interestingly, the core AI model and its use remain the same across these three scenarios. By that logic, a random forest could be called an ‚Äúagent‚Äù for decision trees. We've already been through phases like this where neural networks evolved from one model to another. Like recommender systems emerged with collaborative filtering, contextual bandits, and orchestration of multiple models. But through it all, it was just ‚ÄúAI‚Äù or ‚ÄúReccoAI‚Äù. Yet when I ask an LLM to 'create a table with differences between AI recommender and Recommender AI,' it gives me an answer. Does it make sense? Yes. Does it help? No. I've always believed that ‚ÄúGenerative AI‚Äù is itself a misnomer. Many use cases under that label aren‚Äôt even generating anything. They‚Äôre either doing classification or entity extraction. In our rush to coin fancy new terms, we‚Äôre often confusing systems with components.",
		"engagement": 106,
		"Influencer": "Nitin Aggarwal"
	},
	{
		"text": "MLOps has evolved significantly over the past 5+ years, appearing under different names such as AIOps, GenAIOps, and AgenticOps. However, the fundamental features and challenges remain somewhat similar: model management, versioning, feedback with QA, balancing training versus inference, human-in-the-loop processes, and more. While terminology has shifted from model training to prompt design to agent design/memory, one key focus remains consistent: building scalable systems and deployments.",
		"engagement": 111,
		"Influencer": "Nitin Aggarwal"
	},
	{
		"text": "Until recently, AI decision-making has focused primarily on capabilities and potential gains. Teams build their business case based on what AI can bring and generate value. But a significant shift is emerging (highlighted by recent memos from Shopify, Duolingo, and others) toward defining what AI cannot do. It feels like a minor change but a big shift in the mindset. In statistical terms, it‚Äôs a reversal of null vs. alternative hypothesis. We're entering an interesting phase where AI products will promise certain capabilities while teams request expansions based on AI's limitations. It‚Äôll not only generate new feature requests that have a direct impact on business but also start giving some good food for thought about what and how jobs will evolve with AI. AI adoption was always a big challenge for organizations, and this might be a top-down way to bring that urgency. Organizations accepting this 'limitations-first' perspective will likely gain competitive advantages by designing workflows that strategically complement AI's capabilities rather than merely optimizing the existing processes.",
		"engagement": 1000,
		"Influencer": "Nitin Aggarwal"
	}
]

